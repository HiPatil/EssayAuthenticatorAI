{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":7232489,"sourceType":"datasetVersion","datasetId":4187922}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport pandas as pd\nimport numpy as np\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:03:44.100481Z","iopub.execute_input":"2023-12-18T19:03:44.101012Z","iopub.status.idle":"2023-12-18T19:03:44.109834Z","shell.execute_reply.started":"2023-12-18T19:03:44.100970Z","shell.execute_reply":"2023-12-18T19:03:44.107952Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"csv_path = \"/kaggle/input/combined-llm-essay/train_essays_combined.csv\"\ndf = pd.read_csv(csv_path)\nprint(len(df))","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:03:45.096915Z","iopub.execute_input":"2023-12-18T19:03:45.097535Z","iopub.status.idle":"2023-12-18T19:03:45.186065Z","shell.execute_reply.started":"2023-12-18T19:03:45.097485Z","shell.execute_reply":"2023-12-18T19:03:45.185135Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"2078\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data Preprocessing (Vectorization)\n# Replace with your preprocessing logic\nvectorizer = CountVectorizer(max_features=1000)\nX = vectorizer.fit_transform(df['text']).toarray()\ny = df['generated'].values","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:03:29.914776Z","iopub.execute_input":"2023-12-18T19:03:29.915623Z","iopub.status.idle":"2023-12-18T19:03:31.293170Z","shell.execute_reply.started":"2023-12-18T19:03:29.915576Z","shell.execute_reply":"2023-12-18T19:03:31.291655Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1,1),\n                             tokenizer=lambda x: re.findall(r'[^\\W]+', x),\n                             token_pattern=None,\n                             strip_accents='unicode')\nX = vectorizer.fit_transform(df['text']).toarray()\ny = df['generated'].values","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:07:45.675679Z","iopub.execute_input":"2023-12-18T19:07:45.678527Z","iopub.status.idle":"2023-12-18T19:07:48.244870Z","shell.execute_reply.started":"2023-12-18T19:07:45.678448Z","shell.execute_reply":"2023-12-18T19:07:48.243714Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Convert to PyTorch tensors\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32)\nprint(X_tensor.shape, y_tensor.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:07:48.247305Z","iopub.execute_input":"2023-12-18T19:07:48.248394Z","iopub.status.idle":"2023-12-18T19:07:48.498737Z","shell.execute_reply.started":"2023-12-18T19:07:48.248348Z","shell.execute_reply":"2023-12-18T19:07:48.497783Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"torch.Size([2078, 17994]) torch.Size([2078])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Splitting data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:07:51.605809Z","iopub.execute_input":"2023-12-18T19:07:51.606349Z","iopub.status.idle":"2023-12-18T19:07:51.732021Z","shell.execute_reply.started":"2023-12-18T19:07:51.606308Z","shell.execute_reply":"2023-12-18T19:07:51.730268Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:07:52.078427Z","iopub.execute_input":"2023-12-18T19:07:52.079016Z","iopub.status.idle":"2023-12-18T19:07:52.086852Z","shell.execute_reply.started":"2023-12-18T19:07:52.078968Z","shell.execute_reply":"2023-12-18T19:07:52.085180Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"torch.Size([1662, 17994]) torch.Size([416, 17994])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create Dataset and DataLoader for batch processing\ntrain_data = TensorDataset(X_train, y_train)\ntest_data = TensorDataset(X_test, y_test)\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:07:52.746492Z","iopub.execute_input":"2023-12-18T19:07:52.747391Z","iopub.status.idle":"2023-12-18T19:07:53.165519Z","shell.execute_reply.started":"2023-12-18T19:07:52.747351Z","shell.execute_reply":"2023-12-18T19:07:53.164529Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Neural Network Definition\nclass TextClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(TextClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.sigmoid(self.fc3(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:08:01.255943Z","iopub.execute_input":"2023-12-18T19:08:01.256583Z","iopub.status.idle":"2023-12-18T19:08:01.272128Z","shell.execute_reply.started":"2023-12-18T19:08:01.256533Z","shell.execute_reply":"2023-12-18T19:08:01.269961Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Model Initialization\nmodel = TextClassifier(input_size=17994)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:08:01.814724Z","iopub.execute_input":"2023-12-18T19:08:01.815287Z","iopub.status.idle":"2023-12-18T19:08:01.848360Z","shell.execute_reply.started":"2023-12-18T19:08:01.815245Z","shell.execute_reply":"2023-12-18T19:08:01.847030Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Training Loop\nfor epoch in range(10):  # number of epochs\n    for inputs, labels in train_loader:\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:08:05.532680Z","iopub.execute_input":"2023-12-18T19:08:05.533961Z","iopub.status.idle":"2023-12-18T19:08:12.429988Z","shell.execute_reply.started":"2023-12-18T19:08:05.533908Z","shell.execute_reply":"2023-12-18T19:08:12.428743Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.42881301045417786\nEpoch 2, Loss: 0.051531240344047546\nEpoch 3, Loss: 0.005774134770035744\nEpoch 4, Loss: 0.003481342690065503\nEpoch 5, Loss: 0.0014595434768125415\nEpoch 6, Loss: 0.00190728681627661\nEpoch 7, Loss: 0.0008285972289741039\nEpoch 8, Loss: 0.0012144295033067465\nEpoch 9, Loss: 0.0006635321769863367\nEpoch 10, Loss: 0.0004038279876112938\n","output_type":"stream"}]},{"cell_type":"code","source":"# Model Evaluation\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for inputs, labels in test_loader:\n        outputs = model(inputs)\n        predicted = (outputs.squeeze() > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy: {100 * correct / total}%')","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:08:22.429645Z","iopub.execute_input":"2023-12-18T19:08:22.430177Z","iopub.status.idle":"2023-12-18T19:08:22.490007Z","shell.execute_reply.started":"2023-12-18T19:08:22.430137Z","shell.execute_reply":"2023-12-18T19:08:22.488714Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Accuracy: 99.75961538461539%\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:08:26.301226Z","iopub.execute_input":"2023-12-18T19:08:26.301654Z","iopub.status.idle":"2023-12-18T19:08:26.315653Z","shell.execute_reply.started":"2023-12-18T19:08:26.301618Z","shell.execute_reply":"2023-12-18T19:08:26.314243Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"/kaggle/input/combined-llm-essay/train_essays_combined.csv\n/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\n/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\n/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\n/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nprint(len(df_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:08:27.078478Z","iopub.execute_input":"2023-12-18T19:08:27.079605Z","iopub.status.idle":"2023-12-18T19:08:27.096374Z","shell.execute_reply.started":"2023-12-18T19:08:27.079540Z","shell.execute_reply":"2023-12-18T19:08:27.094442Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"3\n","output_type":"stream"}]},{"cell_type":"code","source":"X_test = vectorizer.transform(df_test['text']).toarray()\nprint(X_test)\n# # Model Evaluation\nmodel.eval()\npredictions = []\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for inputs in X_test:\n        inputs = torch.Tensor(inputs)\n        outputs = model(inputs)\n        predictions.append(outputs.cpu().detach().numpy())\npredictions = np.array(predictions).squeeze()\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:08:28.641486Z","iopub.execute_input":"2023-12-18T19:08:28.642194Z","iopub.status.idle":"2023-12-18T19:08:28.659573Z","shell.execute_reply.started":"2023-12-18T19:08:28.642157Z","shell.execute_reply":"2023-12-18T19:08:28.658269Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n[0.40898916 0.40898916 0.40898916]\n","output_type":"stream"}]},{"cell_type":"code","source":"submission_df = pd.DataFrame({\n    'id': df_test['id'],\n    'generated': predictions\n})","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:08:35.770721Z","iopub.execute_input":"2023-12-18T19:08:35.771575Z","iopub.status.idle":"2023-12-18T19:08:35.778813Z","shell.execute_reply.started":"2023-12-18T19:08:35.771522Z","shell.execute_reply":"2023-12-18T19:08:35.777841Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T19:08:37.342823Z","iopub.execute_input":"2023-12-18T19:08:37.344016Z","iopub.status.idle":"2023-12-18T19:08:37.352429Z","shell.execute_reply.started":"2023-12-18T19:08:37.343963Z","shell.execute_reply":"2023-12-18T19:08:37.351223Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}